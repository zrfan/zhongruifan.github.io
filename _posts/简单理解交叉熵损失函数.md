---
layout:     post
title:      简单理解交叉熵损失函数
subtitle:   简单理解交叉熵损失函数
date:       2019-02-14
author:     fzr
header-img: 
catalog: true
tags:
    - ML
---
## 简单理解交叉熵损失函数

说到交叉熵损失函数（Cross Entropy Loss），就想到公式：$L=-[ylog \hat y + (1-y)log(1- \hat y)]$

大多数情况下，这个交叉熵函数直接拿来用即可，但是知其然还要知其所以然，关于它是怎么推导而来的，为什么它能表征真实样本标签和预测概率之间的差值？交叉熵函数是否有其他变种？本文将从这几个问题总结交叉熵损失函数

#### 1. 交叉熵损失函数的数学原理

二分类问题中，一般使用标签[0, 1]表示负类和正类，如逻辑回归（Logistic Regression），神经网络（Neural Network）等。

模型的预测值通常会经过Sigmoid函数，输出一个概率值，这个概率值反映了预测为正类的可能性：概率越大，可能性越大

Sigmoid函数的表达式：$g(s)= \frac{1}{ (1+e^{-s}) }$   sigmoid是一个S形状的函数

其中s是模型上一层的输出，Sigmoid函数有这样的特点：s=0时，g(s) = 0.5; s>>0时，$g\approx1$, s<<0时，$g \approx 0$。

显然，g(s)将前一级的线性输出映射到[0, 1]之间的数值概率上， 作为模型预测输出。

Sigmoid函数的输出表征了当前样本标签为1的概率：$\hat y = P(y=1|x)$

则当前样本标签为0的概率为：$1-\hat y = P(y=0|x)$

从极大似然的角度出发，把上面两种情况整合到一起，得到：$P(y|x)=\hat y ^ {y} * (1-\hat y)^{1-y}$

当真实样本标签y=0时，概率为$P(y=0|x)=1-\hat y$

当真实样本标签y=1时，概率为$P(y=1|x)=\hat y$

整合之后的概率表达式跟之前完全一致

模型设计的目标是P(y|x)越大越好，首先对P(y|x)引入log函数，因为log运算并不会影响函数本身的单调性。得到：$logP(y|x)=log(\hat y ^ {y} * (1-\hat y) ^ {1-y}) = ylog\hat y + (1-y)log(1-\hat y)$

我们希望logP(y|x)越大越好，即-logP(y|x)越小越好。根据此目标设置损失函数Loss=-logP(y|x)，

得到损失函数：$L=-[ylog\hat y + (1-y)log(1-\hat y)]$

至此，得到了单个样本的损失函数，如果是计算N个样本的总损失函数，则需要将N个Loss叠加起来：$L =- \sum_{i=1}^{N}y^{(i)}log\hat y^{(i)} + (1- y^{i})log(1-\hat y ^ {(i)})$

以上就是交叉熵损失函数的完整推导过程

#### 2. 交叉熵损失函数的现实意义

单个样本的交叉熵损失函数：$L=-[y log\hat y + (1-y)log(1-\hat y) ]$

当y=1时，$L=- log \hat y$ ,对数函数是单调递增的，因此L对 $ \hat y$ 是单调递减函数。即预测输出值$ \hat y$越大（靠近1），损失函数值L越小，预测输出值 $ \hat y$ 越小（接近0）,损失函数值L越大，符合实际需要

当y=0时，$L=-log(1- \hat y)$, L对 $ \hat y $ 是单调递增函数。即预测输出值 $ \hat y$ 越小（接近0），损失函数值L越小，预测输出值 $ \hat y​$ 越大（接近1），损失函数值L越大，同样符合实际需要

无论真实样本标签y是0还是1，L都表征了预测输出与y的差距

而且由于log函数本身的特性所致，预测输出与y差得越多，L的值越大，即对当前模型的”惩罚“越大，且是非线性增大，类似指数增长的级别，这样的好处是模型会倾向于让预测输出更接近真实样本标签y

#### 3. 交叉熵损失函数的变形

假设真实样本的标签为+1和-1，分别表示正类和负类。

有个已知的背景是Sigmoid函数具有如下性质：$1-g(s) = g(-s)$

当y=+1时，得到 $P(y=+1|x) = g(s)$

当y=-1时，得到 $P(y=-1|x) = 1-g(s) = g(-s)$

把上述两个式子整合到一起，得到：

$P(y|x) = g(ys)$

接下来，同样引入log函数，得到： $logP(y|x) = log g(ys)$

模型的目标是让概率值最大，即负数最小。所以定义损失函数为: $L=-logg(ys)$

g是一个sigmoid函数，即： $L=-logg(ys) = -log \frac {1} {1+e^{-ys}} = log(1+e^{-ys})$

L 即是单个样本的交叉熵损失函数，如果是N个样本，则有： $L=\sum_{i=1}^{N} log(1+e^{-ys})$

然后从实际函数来理解，当y=+1时， $L=log(1+e^{-s})$ , L对上一层得分函数s是单调递减函数，s越大（越接近1），损失函数L越小，s越小（接近0），损失函数越大

当y=-1时， $L=log(1+e^s)​$， L对上一层得分函数s是单调递增函数，s越小（接近0），损失函数越小，s越大（接近1），损失函数越大
